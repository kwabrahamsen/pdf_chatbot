import os
from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# ğŸ“ Hvor vi lagrer eller henter den eksisterende vektorindeksen
INDEX_DIR = "index_storage"

# ğŸ§  Sett opp embeddings-modellen
embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ğŸ¤– Sett opp sprÃ¥kmodellen med stÃ¸tte for streaming
llm = Ollama(model="dolphin-mistral")

# ğŸ“¦ Last indeksen hvis den finnes, ellers bygg den fra PDF-er
if os.path.exists(INDEX_DIR):
    print("ğŸ”„ Laster eksisterende indeks ...")
    storage_context = StorageContext.from_defaults(persist_dir=INDEX_DIR)
    index = load_index_from_storage(storage_context, embed_model=embed_model)
else:
    print("ğŸ“„ Leser PDF-er og lager ny indeks ...")
    documents = SimpleDirectoryReader("pdf_folder").load_data()
    index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
    index.storage_context.persist(persist_dir=INDEX_DIR)
    print("âœ… Indeks lagret.")

# ğŸ” GjÃ¸r klar en query engine med streaming aktivert
query_engine = index.as_query_engine(llm=llm, streaming=True)

# ğŸ’¬ Interaktiv kommandolinje
print("\nğŸ§  Klar! Skriv inn spÃ¸rsmÃ¥l. Skriv 'exit' for Ã¥ avslutte.\n")

while True:
    try:
        query = input("â“ SpÃ¸rsmÃ¥l: ").strip()
        if query.lower() in {"exit", "quit"}:
            print("ğŸ‘‹ Avslutter...")
            break
        if not query:
            continue

        print("\nğŸ’¬ Svar:\n")
        response_stream = query_engine.query(query)

        # ğŸ“¡ Stream token-for-token
        for token in response_stream.response_gen:
            print(token, end="", flush=True)
        print("\n")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ Avslutter...")
        break
    except Exception as e:
        print(f"\nâš ï¸ Feil: {e}\n")